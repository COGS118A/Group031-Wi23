{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Zaki Ahmed\n",
    "- Shay Samat\n",
    "- Aditya Tomar\n",
    "- Akhil Vasanth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to create a machine learning model that can predict whether an HB-1 visa will get certified or denied based on the company sponsoring the applicant. The dataset we are using for this project contains 25 columns of data spanning from 2011-2018 filed by various companies and contains features such as the employer and work’s state and location, as well as SOC code, information about wages, and many other relevant details. We will be performing EDA on this dataset to better understand which features are more or less important in determining whether an application is certified or denied. We plan on using ML models, Random Forest, Logistic Regression, SVM, and KNN to build a predictive model that can classify H1-B applications based on the sponsoring company. The success of our models will be measured by using F1 score and ROC-AUC with accuracy calculated for comparison purposes. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "H1-B visas is a very important program that allows companies in the U.S to hire specialized and talented foreign workers in technical positions<a name=\"mpinote\"></a>[<sup>[1]</sup>](#mpinote). This is extremely important for companies when they want to fill key roles in their organizations. Though many people understand the importance of this visa type there are those who see threats. One of the biggest threats is that it can displace workers and depress wages. \n",
    "The USCIS provides a set of guidelines for applying for the H1-B and other Visa types but more importantly, it provides a set of rules that employers must follow. \n",
    "With many people from outside of the U.S. wanting to work here, there is plenty of demand for ways to predict if you can get an H1B visa. So, there has been some research that has aimed to predict whether an employee will have their H1B application certified or not. \n",
    "One research paper<a name=\"iqbalnote\"></a>[<sup>[2]</sup>](#iqbalnote) used primarily characteristics about the employee and their job position to predict this. The features were the employee name, petition year, SOC code, job name, wage, full-time position(boolean), and work site. It used K-Means Clustering to analyze the dataset and tested two classifiers, Random Forest Classifier and Logistic Regression; it found that Logistic Regression classifies better with an accuracy score of 87%, while Random Forest Classifier had an accuracy score of 72%\n",
    "\n",
    "Another research article<a name=\"rananote\"></a>[<sup>[3]</sup>](#rananote) used characteristics of the applicant, the company, and industry they were to create a prediction model that accurately predicts whether an H1B application will be certified, denied, withdrawn or certified withdrawn. This research team used seven ML models including, Decision Tree, SVM, Neural Network, Linear Regression, Naive Bayes, Random Forest, and C5.0, as well as a machine ensemble method (combining all the models to make a better classifier). The models are trained and tested on applications/petitions from 2015 to 2017 and utilizes 20 features. These 20 features included information about the employer and worksite location, wages, H1-B dependence and SOC name (names for different occupation classifications). For all models, train test split is 70-30 and validation of the models is done with 10-fold cross validation. The results from this team states that C5.0 had best single model accuracy, 94.62%, but machine ensemble method had 95.4% accuracy.\n",
    "\n",
    "Our final research paper<a name=\"paulnote\"></a>[<sup>[4]</sup>](#paulnote) we found focuses on trying to come to a conclusion of why there was a surge in denial rates after the 2017 “Buy American and Hire American” Executive Order (EO). This article uses 4 ML models, Logistic Regression, XGBoost, Linear Discriminant, and Naïve Bayes. The validation of these models were computed through 5-fold cross validation. The model uses both firm and applicant characteristics. Applicant characteristic features span from education level (degree), occupation, and nationality. The firm features consist of the state in which they are in, the location of their headquarters, whether they are publicly traded, and H1 Dependent. Out of the 4 ML models used, it found that linearly dependent models outperformed XGBoost. Logistic Regression had the best accuracy, with 93% in the pre-treatment, and 82% in the post-treatment, Linear Discriminant had 91% in the pre-treatment and 81% in the post-treatment, Naïve Bayes had 92% in the pre-treatment and 81% in the post-treatment, and finally XGBoost had 90% in the pre-treatment and 80% in the post-treatment. Overall these results are fairly similar, however XGBoost takes into account potential non-linearities decision-boundaries, which is why for the rest of the paper they focused on the best linear model, logistic regression and also XGBoost.\n",
    "In conclusion, the H1-B visa program is one of the most critical parts of the US economy because it allows foreign workers to fill in technical roles. Although we see this as a benefit, the program has received lots of criticism for some displacement of American workers and low wages. Due to this the USCIS created a list of guidelines to apply for the H1-B visa program. Based on these guidelines and access to the datasets, many machine learning models were created to predict the chance of approval for this visa. What makes our approach different is that we will be focusing on employer/company metrics, rather than employee/applicant metrics as focused on in the articles above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Problem statement: How accurately can we predict whether an H1B application will be certified or denied, based on the size, location, industry, and sales volume of the company sponsoring the employee and the details of the job being applied for (wage, job title, full time status)? What classifier can predict H-1B acceptance/denial with the greatest accuracy score? \n",
    "\n",
    "\n",
    "Our problem statement’s motivation is to find whether the metrics of the H1B sponsoring employer have a significant predictive value on approval odds of the H1B application and whether using this information can generate better predictions than not using them. All research data we have seen thus far do not use demographic/financial info of the company, rather they use demographic data from applicants. We think this is likely an oversight as H-1B applications are submitted by companies rather than individuals so we hypothesize company metrics might be more useful than individual demographic information in the context of H-1B applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Our primary dataset will be the H-1B Non-Immigrant Labour Visa dataset (Source: H-1B Non-Immigrant Labour Visa | Kaggle) which contains information on H-1B visa applications made by US employers with the primary feature of interest being “case_status” which includes approval or denial (or other) for each application. There are 3.36 million observations or applications with 25 variables/columns including information on employer location, job information (title, wage, full time for example)  for the prospective employee, and the dates of the application. To reiterate, the most important feature is the feature “case_status” which is stored as a categorical string encoding whether or not the case was approved. Following this, the next most important feature will be employer name because it will allow us to generate additional features with information on each company’s size, finances, and more. The boolean feature “emp_h1b_dependent” which encodes whether or not the employer is dependent on H-1B workers for its workforce will likely have an insignificant weight/impact on the prediction as there will likely be a correlation. We have not fully decided on the secondary dataset with information on employers but we have found one which crucially has information on “12 million companies” with features of note for our purposes being stock ticker, white collar percentage, employee number, annual sales, company age (ARC Dataset). These features and other features of companies will give our future trained model additional information to base predictions off of, hopefully improving prediction. To combine these datasets we would need to clean the company dataset of any missing values (either delete entire rows or use some imputation method) and then do an inner join on the company name features in both datasets. This will likely require some string manipulation as the same company could be written slightly differently across the two datasets. This join would ideally generate new columns for each application with extra information on the employer associated with the application (observation). \n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the sake of being able to compare our H-1B classification models, we will be using accuracy as our first metric.  Accuracy is simply the proportion of correctly classified instances out of all instances in the dataset. Mathematically, it can be represented as: (TP + TN) / (TP + TN + FP + FN) However, accuracy will not be our primary metric for overall evaluation because the classes are imbalanced. Due to this we will be using F1-score and Area Under the ROC Curve (AUC-ROC) to evaluate overall performance. We feel the costs of false positives and false negatives are roughly equivalent in the context of our model being purely for analytical purposes and not for commercial use. The F1-score is the harmonic mean of precision and recall, and provides a balanced measure of the model's performance and can be represented as: T2 * (precision * recall) / (precision + recall). The AUC-ROC measures the performance of the model across all possible classification thresholds, and is useful when the threshold for classification is not known. In the context of H-1B visa application prediction, the goal will be to identify a threshold that maximizes both precision and recall since we assume them to be equally important. The AUC-ROC also measures the model's performance across all possible classification thresholds, and is calculated by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold values. The AUC-ROC provides a single number (between 0 and 1) that summarizes the model's performance across all thresholds, making it a useful metric for overall evaluation. F1-Score and AUC-ROC will be used for model evaluation while accuracy will primarily be used for comparison with past models with the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the CSV files\n",
    "csv_files = [\"data/h1b_pt\"+str(i)+\".csv\" for i in list(range(1,9))]\n",
    "\n",
    "# Create an empty list to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file, read it into a DataFrame, and append it to the list\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames together into a single DataFrame\n",
    "h1b = pd.concat(dfs, ignore_index=True)\n",
    "h1b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h1b.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a bar chart \n",
    "\n",
    "#first, get all samples where the cases were certified and all samples where the cases weren't certified \n",
    "\n",
    "h1b_cert = df.loc[df['case_status'] == \"C\"]\n",
    "h1b_noncert = df.loc[df['case_status'] != 'C']\n",
    "h1b_grouped_year = h1b.groupby('case_year').count()['case_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1b_grouped_year.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can view how many missing cases there are so far in each column to help identify what to drop and what to keep\n",
    "\n",
    "h1b.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe and drop columns that aren't of interest to us\n",
    "\n",
    "drop_cols = [\n",
    "     \"emp_country\",\n",
    "    \"pw_level\", \n",
    "    \"soc_name\", \n",
    "    \"wage_to\", \n",
    "    \"lat\", \n",
    "    \"lng\"\n",
    "]\n",
    "\n",
    "data = h1b.drop(drop_cols, axis=1) \n",
    "\n",
    "cols = data.columns\n",
    "\n",
    "data.head()\n",
    "\n",
    "#now, check the missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_clean = data.replace('-', np.nan).dropna(axis=0)\n",
    "\n",
    "#we will change case_status, our labels, to be C(confirmed) and NC(not confirmed) \n",
    "#W(withdrawn), D(denied), and CW(certified-withdrawn) are all considered to be NC because in all these scenarios, \n",
    "#the applicant will not be going to the US to work on an H1B Visa\n",
    "data_clean['case_status'] = data_clean['case_status'].replace(['W','D','CW'],'NC')\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['emp_willful_violator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create confusion  matrix\n",
    "le = LabelEncoder()\n",
    "a = le.fit_transform(data_clean['case_status'])\n",
    "b = le.fit_transform(data_clean['emp_willful_violator'])  \n",
    "conf_matrix = confusion_matrix(a,b)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False,\n",
    "            xticklabels=[\"N\", \"Y\"], yticklabels=[\"C\", \"NC\"])\n",
    "plt.xlabel(\"Willful Violator\")\n",
    "plt.ylabel(\"Case_status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_decision_stump_classifier(row):\n",
    "    if row['emp_willful_violator']=='N':\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'NC'\n",
    "    \n",
    "dec_stump_preds = data_clean.apply(simple_decision_stump_classifier, axis=1)\n",
    "true = data_clean['case_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.count_nonzero([(true==\"C\") & (dec_stump_preds==\"C\")])\n",
    "FP = np.count_nonzero([(true==\"NC\") & (dec_stump_preds==\"C\")])\n",
    "TN = np.count_nonzero([(true==\"NC\") & (dec_stump_preds==\"NC\")])\n",
    "FN = np.count_nonzero([(true==\"C\") & (dec_stump_preds==\"NC\")])\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# calculate precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# calculate recall\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# calculate f1 score\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_case_idx = data_clean.set_index(['emp_name','case_status'])\n",
    "emp_h1bdep_idx = data_clean.set_index(['emp_name','emp_h1b_dependent'])\n",
    "\n",
    "case_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','case_status'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "h1bdep_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','emp_h1b_dependent'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "case_by_emp_tots = case_by_emp.sum(axis=1)\n",
    "emp_case_idx = data_clean.set_index(['emp_name','case_status'])\n",
    "emp_h1bdep_idx = data_clean.set_index(['emp_name','emp_h1b_dependent'])\n",
    "\n",
    "case_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','case_status'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "h1bdep_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','emp_h1b_dependent'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "case_by_emp_tots = case_by_emp.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_by_emp.loc['\"K\" LINE AMERICA']['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for making bar plot of a feature\n",
    "\n",
    "def makeplot(feature):\n",
    "    counts = data_clean.groupby(feature)['case_status'].value_counts()\n",
    "    print(counts)\n",
    "    counts = counts.unstack()\n",
    "\n",
    "    ax = counts.plot.bar(stacked=True,rot=0)\n",
    "\n",
    "    # totals = counts.sum(axis=1)\n",
    "    # for i, bar in enumerate(ax.containers):\n",
    "    #     for rect in bar:\n",
    "    #         # get the height and width of the bar\n",
    "    #         height = rect.get_height()\n",
    "    #         width = rect.get_width()\n",
    "    #         # compute the proportion\n",
    "    #         prop = height / totals[i]\n",
    "    #         # add the proportion label\n",
    "    #         ax.text(rect.get_x() + width / 2, rect.get_y() + height / 2, f'{prop:.0%}', ha='center', va='center')\n",
    "    \n",
    "    #         print(i,bar)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#make plots for 5 features: emp_h1b_dependent, emp_willful_violator, full_time_position, case_year, \n",
    "#makeplot('emp_h1b_dependent')\n",
    "#makeplot('emp_willful_violator')\n",
    "#makeplot('full_time_position')\n",
    "makeplot('case_year')  #case year has only two years now, so I don't know what to do about that\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make histogram for prevailing wage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to get a new field, soc_field, that gives us more broad categories of occupations that\n",
    "#encompass multiple types of jobs\n",
    "\n",
    "data_clean['soc_field'] = data_clean['soc_code'].str[:2]\n",
    "data_clean['soc_code'].unique()\n",
    "#cleaning up the soc_field column\n",
    "#both of the occupations represented by these letters fall under 15, \n",
    "data_clean['soc_field'] = data_clean['soc_field'].replace(['CO', 'SO'], '15') \n",
    "\n",
    "#the occupations represented by these letters fall under 17\n",
    "data_clean['soc_field'] = data_clean['soc_field'].replace(['EL', 'EN', 'ME'], '17')\n",
    "data_clean['soc_field'] = data_clean['soc_field'].replace('1-', '31') #1- is a \n",
    "data_clean['soc_field'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['case_status'] = data_clean['case_status'].replace(['W','D','CW'],'NC')\n",
    "counts = data_clean.groupby('emp_h1b_dependent')['case_status'].value_counts()\n",
    "counts = counts.unstack()\n",
    "ax = counts.plot.bar(stacked=True,rot=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.groupby('emp_name')['case_status'].value_counts().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data_clean.groupby('emp_willful_violator')['case_status'].value_counts()\n",
    "counts = counts.unstack()\n",
    "ax = counts.plot.bar(stacked=True,rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_factors = {'Y':1,'H':2080,'M':12,'W':52,'BW':26}\n",
    "data_final['pw_yearly'] = data_final.apply(lambda row: row['prevailing_wage'] * conversion_factors[row['pw_unit']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['wage_from_yearly'] = data_final.apply(lambda row: row['wage_from'] * conversion_factors[row['wage_unit']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_diff = ((data_final['pw_yearly']-data_final['wage_from_yearly'])/data_final['wage_from_yearly'])*1.0\n",
    "wage_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['case_status'] = data['case_status'].replace(['W','D','CW'],'NC')\n",
    "data['case_status'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.case_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# Create confusion  matrix\n",
    "a = data_clean['case_status']\n",
    "b = data_clean['emp_willful_violator']\n",
    "conf_matrix = confusion_matrix(a,b)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False,\n",
    "            xticklabels=[\"N\", \"Y\"], yticklabels=[\"NC\", \"C\"])\n",
    "plt.xlabel(\"Willful Violator\")\n",
    "plt.ylabel(\"Case_status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# Create confusion  matrix\n",
    "a = data_clean['case_status']\n",
    "b = data_clean['emp_h1b_dependent']\n",
    "conf_matrix = confusion_matrix(a,b)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False,\n",
    "            xticklabels=[\"N\", \"Y\"], yticklabels=[\"0\", \"1\"])\n",
    "plt.xlabel(\"h1b_dependent\")\n",
    "plt.ylabel(\"Case_status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our approach to solving our classification problem will be to generate multiple models, refine those models’ hyperparameters, and then select the best model for final prediction evaluation based on test data unseen by any of the models to see generalization performance. We will first do an 80-20 train(/valid)-test split to allow for better model evaluation later since our dataset is so large. To solve our classification problem we will use several models from SciKit Learn and select the best model after finding the best hyperparameters for each of the models using gridsearch. The models we have in mind for classification are Random Forest, Logistic Regression, SVM, and KNN. Since we do not know the distribution of our high dimensional dataspace, we will be thinking of these models but our decision may be and will likely be influenced by what we find in our initial EDA of the data and EDA will also provide insight into feature importance. Due to the high dimensionality of the data, PCA will likely need to be performed as dimensionality reduction before being input into training the models to improve performance. Each model’s best hyperparameters will be found using random search and then gridsearch after which each model will be compared against each other by using k-fold cross validation on the train/validation data split (80% of the data). After which each model will be tested on the completely unseen testing data split (20%) to determine generalization performance of each model. Our best resulting model will be selected with a heavy emphasis on generalization performance. We will be aiming to have better performance than the performance of previous H-1B prediction algorithms we found and will find in our research- the best of which we have seen has an accuracy of 96% for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ethics and privacy concerns with the dataset we have chosen regarding H1-B Non Immigrant Labour Visa. The dataset that we have is anonymous, however, with the features described, employees could be re-identified, as the dataset contains information regarding the city, state, and zip code of both the employer and employee. However, to address this, we will refrain from using information of the employee in our project. The dataset also avoids proxy discrimination, as we are not using features such as race, origin, sex, and age, rather using attributes from the company. As far as bias, since we are mainly going to be using columns regarding the employer and the case status, and disregarding columns related to the employee. As we are reducing the number of columns used, we could be missing some potential variables that could lead to biased predictions in our results regarding the approval of visas. To address this, we will choose models that are less prone to bias and use evaluation metrics that mitigate bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Team Expectation 1: Communication\n",
    "    * If you have an issue or are confused about something, let one of us or all of us know so we can figure it out together.\n",
    "* Team Expectation 2: Presence\n",
    "    * When the team is meeting, we understand that not everyone can meet in-person/at the same time, but we expect that the members who cannot make it are active on the doc, and keeping up with what is being discussed.\n",
    "* Team Expectation 3: Schedule\n",
    "    * We are aiming to stick to the schedule we have made and agreed upon and are expecting everyone to follow and hold each other accountable.\n",
    "* Team Expectation 4: Being Active\n",
    "    * We expect all team members to check our discord group and Google Docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/27  |  9 AM|  Peer Review Proposal DUE  | Wrap up Peer Review Proposals, discuss feature selections and find dataset that contains metrics needed for employers | \n",
    "| 3/1  |  9 AM |  Find Dataset for metrics with Employers | Discuss wranging and layout main idea of analytical approach. | \n",
    "| 3/3 | 9 AM  | Begin EDA  | Review/Edit wrangling/EDA  <br/> Work on Checkpoint due 3/8 |\n",
    "| 3/6  | 9 AM | Finish EDA <br/> Work on Checkpoint due 3/8 | Review Final status of EDA and Discuss Analysis Plan <br/> Work on Checkpoint due 3/8|\n",
    "| 3/8  | 9 AM  | Start Project Code <br/> Checkpoint due TODAY| Discuss/edit project code <br/> Complete project |\n",
    "| 3/10  | 9 AM  | Complete analysis <br/> Draft results/conclusion/discussion| Discuss/edit full project |\n",
    "| 3/13  | 9 AM  | Make an outline/checklist of the final project | Discuss any lingering problems/concerns and work on final touches  |\n",
    "| 3/15 | 9 AM  | Address any lingering problems/concerns and work on final touches| Discuss/Address any lingering problems/concerns and work on final touches |\n",
    "| 3/17  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"mpinote\"></a>1.[^](#mpinote): Migration Policy Institute. (2022). H-1B Temporary Skilled Worker Program. Retrieved February 22, 2023, from https://www.migrationpolicy.org/article/h-1b-temporary-skilled-worker-program.<br> \n",
    "<a name=\"iqbalnote\"></a>2.[^](#iqbalnote): M. Tariq Iqbal, M. Irfan Ullah, M. Ahsan Nazir, \"A Hybrid Algorithm for Optimal Placement and Sizing of Distributed Generation in a Distribution System\", IEEE Access, vol. 7, pp. 174089-174102, 2019. doi: 10.1109/ACCESS.2019.2953292. https://ieeexplore.ieee.org/abstract/document/8933628<br>\n",
    "<a name=\"rananote\"></a>3.[^](#rananote): Rana, P., Kumar, S., & Yadav, S. (2019). An allotment of H1B work visa in USA using machine learning. International Journal of Recent Technology and Engineering, 8(3S), 539-545. Available from: https://www.researchgate.net/profile/Prashant-Rana-4/publication/328488339_An_allotment_of_H1B_work_visa_in_USA_using_machine_learning/links/5d70f092a6fdcc9961afad48/An-allotment-of-H1B-work-visa-in-USA-using-machine-learning.pdf.<br>\n",
    "<a name=\"paulnote\"></a>4.[^](#paulnote): Paul, U., & Langlois, A. (2022). Understanding the Determinants of H-1B Decisions: A Machine Learning Approach. Northeastern University, College of Social Sciences and Humanities. Retrieved February 22, 2023, from https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4143882<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "19b17eba0dbd5e4b8827ab8a6192fc0dff7c2985f63f4f278d5b971ef380745d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

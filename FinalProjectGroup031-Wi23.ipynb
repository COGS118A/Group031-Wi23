{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (StandardScaler, Binarizer, OrdinalEncoder, OneHotEncoder)\n",
    "from sklearn.metrics import (confusion_matrix, f1_score, accuracy_score)\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting HB-1 Visa Certification: A Machine Learning Approach Using Employer and Work Information\n",
    "\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Zaki Ahmed\n",
    "- Shay Samat\n",
    "- Aditya Tomar\n",
    "- Akhil Vasanth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to create a machine learning model that can predict whether an HB-1 visa will get certified or denied based on the company sponsoring the applicant. The dataset we are using for this project contains 25 columns of data spanning from 2011-2018 filed by various companies and contains features such as the employer and work’s state and location, as well as SOC code, information about wages, and many other relevant details. We will be performing EDA on this dataset to better understand which features are more or less important in determining whether an application is certified or denied. We plan on using ML models, Random Forest, Logistic Regression, SVM, and KNN to build a predictive model that can classify H1-B applications based on the sponsoring company. The success of our models will be measured by using F1 score and ROC-AUC with accuracy calculated for comparison purposes. \n",
    "\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "H1-B visas is a very important program that allows companies in the U.S to hire specialized and talented foreign workers in technical positions<a name=\"mpinote\"></a>[<sup>[1]</sup>](#mpinote). This is extremely important for companies when they want to fill key roles in their organizations. Though many people understand the importance of this visa type there are those who see threats. One of the biggest threats is that it can displace workers and depress wages. \n",
    "The USCIS provides a set of guidelines for applying for the H1-B and other Visa types but more importantly, it provides a set of rules that employers must follow. \n",
    "With many people from outside of the U.S. wanting to work here, there is plenty of demand for ways to predict if you can get an H1B visa. So, there has been some research that has aimed to predict whether an employee will have their H1B application certified or not. \n",
    "One research paper<a name=\"iqbalnote\"></a>[<sup>[2]</sup>](#iqbalnote) used primarily characteristics about the employee and their job position to predict this. The features were the employee name, petition year, SOC code, job name, wage, full-time position(boolean), and work site. It used K-Means Clustering to analyze the dataset and tested two classifiers, Random Forest Classifier and Logistic Regression; it found that Logistic Regression classifies better with an accuracy score of 87%, while Random Forest Classifier had an accuracy score of 72%\n",
    "\n",
    "Another research article<a name=\"rananote\"></a>[<sup>[3]</sup>](#rananote) used characteristics of the applicant, the company, and industry they were to create a prediction model that accurately predicts whether an H1B application will be certified, denied, withdrawn or certified withdrawn. This research team used seven ML models including, Decision Tree, SVM, Neural Network, Linear Regression, Naive Bayes, Random Forest, and C5.0, as well as a machine ensemble method (combining all the models to make a better classifier). The models are trained and tested on applications/petitions from 2015 to 2017 and utilizes 20 features. These 20 features included information about the employer and worksite location, wages, H1-B dependence and SOC name (names for different occupation classifications). For all models, train test split is 70-30 and validation of the models is done with 10-fold cross validation. The results from this team states that C5.0 had best single model accuracy, 94.62%, but machine ensemble method had 95.4% accuracy.\n",
    "\n",
    "Our final research paper<a name=\"paulnote\"></a>[<sup>[4]</sup>](#paulnote) we found focuses on trying to come to a conclusion of why there was a surge in denial rates after the 2017 “Buy American and Hire American” Executive Order (EO). This article uses 4 ML models, Logistic Regression, XGBoost, Linear Discriminant, and Naïve Bayes. The validation of these models were computed through 5-fold cross validation. The model uses both firm and applicant characteristics. Applicant characteristic features span from education level (degree), occupation, and nationality. The firm features consist of the state in which they are in, the location of their headquarters, whether they are publicly traded, and H1 Dependent. Out of the 4 ML models used, it found that linearly dependent models outperformed XGBoost. Logistic Regression had the best accuracy, with 93% in the pre-treatment, and 82% in the post-treatment, Linear Discriminant had 91% in the pre-treatment and 81% in the post-treatment, Naïve Bayes had 92% in the pre-treatment and 81% in the post-treatment, and finally XGBoost had 90% in the pre-treatment and 80% in the post-treatment. Overall these results are fairly similar, however XGBoost takes into account potential non-linearities decision-boundaries, which is why for the rest of the paper they focused on the best linear model, logistic regression and also XGBoost.\n",
    "In conclusion, the H1-B visa program is one of the most critical parts of the US economy because it allows foreign workers to fill in technical roles. Although we see this as a benefit, the program has received lots of criticism for some displacement of American workers and low wages. Due to this the USCIS created a list of guidelines to apply for the H1-B visa program. Based on these guidelines and access to the datasets, many machine learning models were created to predict the chance of approval for this visa. What makes our approach different is that we will be focusing on employer/company metrics, rather than employee/applicant metrics as focused on in the articles above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "How accurately can we predict whether an H1B application will be certified or denied, based on the size, location, industry, and sales volume of the company sponsoring the employee and the details of the job being applied for (wage, job title, full time status)? What classifier can predict H-1B acceptance/denial with the greatest accuracy score? \n",
    "\n",
    "Our problem statement’s motivation is to find whether the metrics of the H1B sponsoring employer have a significant predictive value on approval odds of the H1B application and whether using this information can generate better predictions than not using them. All research data we have seen thus far do not use demographic/financial info of the company, rather they use demographic data from applicants. We think this is likely an oversight as H-1B applications are submitted by companies rather than individuals so we hypothesize company metrics might be more useful than individual demographic information in the context of H-1B applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- H-1B Non-Immigrant Labour Visa\n",
    "- Link via Kaggle: https://www.kaggle.com/datasets/thedevastator/h-1b-non-immigrant-labour-visa\n",
    "- Contains information on H-1B visa applications made by US employers\n",
    "- **There are 3.36 million observations or applications with 25 variables/columns**\n",
    "- Primary feature of interest is “case_status”\n",
    "- Includes whether the application is certified, certified withdrawn or withdrawn\n",
    "- It is a finite set of discrete values that are not numerical\n",
    "- Stored as a categorical string encoding whether or not the case was approved\n",
    "The features that are pertinent can be divided into 4 broad groups:\n",
    "- Features about Location\n",
    "  - **location** (feature engineered)\n",
    "    - use **emp_zip** and make general geographic locations of regions (NESW)\n",
    "    - Categorical type data\n",
    "- Features about Wage\n",
    "  - Conversion using **wage_unit** (feature engineered)\n",
    "    - **wage_unit** describes hourly or yearly\n",
    "    - Use this to convert all wage values that are hourly to yearly because 93% of values are yearly\n",
    "    - This will allow for the wages to be directly comparable\n",
    "- Features about Employer Data\n",
    "  - **job demand** (feature engineered)\n",
    "    - count the number of applications submitted per employer, the more applications, the more demand\n",
    "  - **emp_h1b_dependent**\n",
    "    - Whether the employer is H-1B dependent or not. (Boolean)\n",
    "    - Categorical type data\n",
    "  - **emp_willful_violator**\n",
    "    - Whether the employer is a willful violator or not. (Boolean)\n",
    "    - Categorical type data\n",
    "- Features about Application\n",
    "  - **Application duration** (feature engineered)\n",
    "    - time it takes for application to be approved/denied\n",
    "    - decision_date - case_submitted\n",
    "    - Continuous numerical data type\n",
    "  - **case_status**\n",
    "    - the status of the case, either approved or denied. (String)\n",
    "    - Categorical type data\n",
    "  - **case_year**\n",
    "    - the year in which the case was submitted. (Integer)\n",
    "    - Ordinal type data\n",
    "  - **full_time_position**\n",
    "    - whether the position is full-time or not. (Boolean)\n",
    "    - Categorical type data\n",
    "  - **soc_field** (feature engineered)\n",
    "    - By taking the first 3 numbers of **soc_code** which denotes the field of the occupation as per the SOC manual by the Department of Labor.\n",
    "    - Link: https://www.bls.gov/soc/2018/soc_2018_definitions.pdf\n",
    "    - We can create fields such as Top Executives, Business Operations Specialists, etc.\n",
    "- Finally, The numerical features will be standard scaled, the categorical variables will be one-hot encoded and the time variables will be date-timed and converted to an ordinal scale.\n",
    "\n",
    "\n",
    "Update on Employer dataset:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- We will no longer be using the employer dataset, we had quite a bit of trouble trying to find a dataset to work with, so our solution was to use feature engineering to create our own features regarding employers.\n",
    "- Since we have created our own features through feature engineering, and these features are sufficient for our analysis, then we no longer need an external dataset that provides information on employers.\n",
    "- The features we have generated are essentially a way of creating new variables that can capture important information about the employers in your dataset, such as their industry, company size, and wage practices.\n",
    "- These new features include, **soc_field**, **company_size**, **Application duration** ,**wage_level**.\n",
    "- By generating these new features, we have essentially created a more granular view of the data that we are working with, allowing us to gain insights that we might not have been able to see otherwise. In addition, by creating these features ourselves, we have more control over the quality and accuracy of the data that we are working with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our approach to solving our classification problem will be to generate multiple models, refine those models’ hyperparameters, and then select the best model for final prediction evaluation based on test data unseen by any of the models to see generalization performance. We will first do an 80-20 train(/valid)-test split to allow for better model evaluation later since our dataset is so large. To solve our classification problem we will use several models from SciKit Learn and select the best model after finding the best hyperparameters for each of the models using gridsearch. The models we have in mind for classification is Random Forest, XGBoost, and AdaBoost. Since we do not know the distribution of our high dimensional dataspace, we will be thinking of these models but our decision may be and will likely be influenced by what we find in our initial EDA of the data and EDA will also provide insight into feature importance. Due to the high dimensionality of the data, PCA will likely need to be performed as dimensionality reduction before being input into training the models to improve performance. Each model’s best hyperparameters will be found using random search and then gridsearch after which each model will be compared against each other by using k-fold cross validation on the train/validation data split (80% of the data). After which each model will be tested on the completely unseen testing data split (20%) to determine generalization performance of each model. Our best resulting model will be selected with a heavy emphasis on generalization performance. We will be aiming to have better performance than the performance of previous H-1B prediction algorithms we found and will find in our research- the best of which we have seen has an accuracy of 96% for example. In addition to the models mentioned earlier, we will also be using Decision Stump and Naive Bayes as our benchmark models. These models are weaker compared to the other models, making them a good starting point to establish a baseline for comparison. Decision Stump is a simple model that makes use of a single feature, emp_willful_violator, to predict whether an application was certified or not, of the input data, making it a good baseline model for comparison with more complex models. We found during our EDA process that 99.9% of our dataset are not wilful violators, so if the company you applied for is a wilful violator, you will most likely get denied. Naive Bayes, on the other hand, is a probabilistic model that assumes that the features of the input data are independent of each other, and despite its simplifying assumption, it has been shown to work well in many real-world classification problems. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the sake of being able to compare H-1B classification models, we will be using accuracy as our first metric. Accuracy is simply the proportion of correctly classified instances out of all instances in the dataset. Mathematically, it can be represented as: $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$ However, accuracy will not be our primary metric for overall evaluation because the classes are imbalanced. Due to this we will be using F1-score and Area Under the ROC Curve (AUC-ROC) to evaluate overall performance. We feel the costs of false positives and false negatives are roughly equivalent in the context of our model being purely for analytical purposes and not for commercial use. In the context of H-1B visa application prediction, false positives refer to cases where the model incorrectly predicts that an application will be approved, when in fact it will be denied. This can be costly for both the applicant and the employer, as they may have already invested time and resources into the application process, only to be denied. False negatives refer to cases where the model incorrectly predicts that an application will be denied, when in fact it will be approved. This can result in missed opportunities for both the applicant and the employer, as they may have been able to secure a visa if the application had been properly approved. The F1-score is the harmonic mean of precision and recall, and provides a balanced measure of the model's performance and can be represented as: $$ F1 = 2 * \\frac{precision * recall}{precision + recall} $$. The AUC-ROC measures the performance of the model across all possible classification thresholds, and is useful when the threshold for classification is not known. In the context of H-1B visa application prediction, the goal will be to identify a threshold that maximizes both precision and recall since we assume them to be equally important. The AUC-ROC also measures the model's performance across all possible classification thresholds, and is calculated by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold values. The equation can be written as:  $$ AUC-ROC = \\int_0^1 TPR(FPR^{-1}(t))dt $$ where $FPR^{-1}(t)$ is the inverse function of the FPR. The AUC-ROC provides a single number (between 0 and 1) that summarizes the model's performance across all thresholds, making it a useful metric for overall evaluation. F1-Score and AUC-ROC will be used for model evaluation while accuracy will primarily be used for comparison with past models with the same goal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Results/EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (StandardScaler, Binarizer, OrdinalEncoder, OneHotEncoder)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection and Cleaning\n",
    "\n",
    "First, we downloaded our data from Kaggle as a set of 8 .csv files. We had to split it into 8 files because of the size of the dataset. Then, we converted the 8 files into a pandas dataframe that we can manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the CSV files\n",
    "csv_files = [\"data/h1b_pt\"+str(i)+\".csv\" for i in list(range(1,9))]\n",
    "\n",
    "# Create an empty list to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file, read it into a DataFrame, and append it to the list\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames together into a single DataFrame\n",
    "h1b = pd.concat(dfs, ignore_index=True)\n",
    "h1b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1b.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we worked on cleaning the dataframe by removing rows with missing/null values, removing columns that we are not interested in, and converting values in some columns for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can view how many missing cases there are so far in each column to help identify what to drop and what to keep\n",
    "\n",
    "h1b.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deleted the columns we are not interested in. This includes:\n",
    "- employer country(emp_country)\n",
    "- prevailing wage level(pw_level)\n",
    "- SOC name(soc_name)\n",
    "- maximum wage for the job position(wage_to)\n",
    "- latitude(lat) and longitude(lng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe and drop columns that aren't of interest to us\n",
    "\n",
    "drop_cols = [\n",
    "     \"emp_country\",\n",
    "    \"pw_level\", \n",
    "    \"soc_name\", \n",
    "    \"wage_to\", \n",
    "    \"lat\", \n",
    "    \"lng\"\n",
    "]\n",
    "\n",
    "data = h1b.drop(drop_cols, axis=1) \n",
    "\n",
    "cols = data.columns\n",
    "\n",
    "data.head()\n",
    "\n",
    "#now, check the missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we dealt with any missing values in our dataset. Since most of the columns with missing values were non-numeric and replacing the missing values with another value may result in inaccurate data, we decided to remove all rows that contained missing values.\n",
    "\n",
    "Additionally, the case_status column in our dataset contains the class labels we want to predict. We changed the values in this column to be:\n",
    "- confirmed(C), meaning the H1B application was certified by the government and not withdrawn\n",
    "- not confirmed(NC), meaning the H1B application was either withdrawn(W), certified then withdrawn(CW), or denied(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_clean = data.replace('-', np.nan).dropna(axis=0)\n",
    "\n",
    "#we will change case_status, our labels, to be C(confirmed) and NC(not confirmed) \n",
    "#W(withdrawn), D(denied), and CW(certified-withdrawn) are all considered to be NC because in all these scenarios, \n",
    "#the applicant will not be going to the US to work on an H1B Visa\n",
    "data_clean['case_status'] = data_clean['case_status'].replace(['W','D','CW'],'NC')\n",
    "\n",
    "data_clean['case_submitted'] = data_clean['case_submitted'].apply(pd.to_datetime)\n",
    "data_clean['decision_date'] = data_clean['decision_date'].apply(pd.to_datetime)\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "Now that our data is cleaned, we can now visualize features of interest with different plots. We used confusion matrices for binary features, bar graphs for binary and categorical features and a histogram for one of our numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plots we explored were confusion matrices. We had two theories: We thought that H1B Dependence and Willful Violator features affected whether or not a case was confirmed or not confirmed. We hypothesized that H1B Dependence would be a good predictor because if a company is dependent on H1B workers. Their applications may be less likely to be accepted because they already have many H1B employees. Additionally, we thoerized that a company being a willful violator of H1B rules would make their future applications extremely likely to be denied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion  matrix for binary features\n",
    "def makeconfmatrix(feature):\n",
    "    \"\"\"\n",
    "    creates a confusion matrix for binary features that plots Case Status and the feature parameter\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    a = le.fit_transform(data_clean['case_status'])\n",
    "    b = le.fit_transform(data_clean[feature])  \n",
    "    conf_matrix = confusion_matrix(a,b)\n",
    "\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False,\n",
    "                xticklabels=[\"N\", \"Y\"], yticklabels=[\"C\", \"NC\"])\n",
    "    \n",
    "    words = feature.split(\"_\")\n",
    "\n",
    "    # capitalize each word and join them back into a string\n",
    "    new_string = \" \".join([word.capitalize() for word in words])\n",
    "    plt.xlabel(new_string)\n",
    "    plt.ylabel(\"Case_status\")\n",
    "    plt.show()\n",
    "\n",
    "makeconfmatrix('emp_h1b_dependent')\n",
    "makeconfmatrix('emp_willful_violator')\n",
    "makeconfmatrix('full_time_position')\n",
    "\n",
    "\n",
    "#now, make a heatmap for the Case Year feature\n",
    "le = LabelEncoder()\n",
    "a = le.fit_transform(data_clean['case_status'])\n",
    "b = le.fit_transform(data_clean['case_year'])  \n",
    "conf_matrix = confusion_matrix(a,b)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False,\n",
    "            xticklabels=[\"2015\", \"2017\"], yticklabels=[\"C\", \"NC\"])\n",
    "\n",
    "plt.xlabel(\"Case Year\")\n",
    "plt.ylabel(\"Case_status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most notable correlation was between willful violator and case status. If an employer was not a willful violator, their chances of having their applications be confirmed were much higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs some data preprocessing and aggregation on a cleaned dataset (data_clean) by first setting two multi-level indices on the dataframe based on 'emp_name' and 'case_status' or 'emp_h1b_dependent'. It then uses these indices to create two new dataframes, case_by_emp and h1bdep_by_emp, which group and count the number of cases for each employer by case status and by H1B dependence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_case_idx = data_clean.set_index(['emp_name','case_status'])\n",
    "emp_h1bdep_idx = data_clean.set_index(['emp_name','emp_h1b_dependent'])\n",
    "\n",
    "case_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','case_status'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "h1bdep_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','emp_h1b_dependent'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "case_by_emp_tots = case_by_emp.sum(axis=1)\n",
    "emp_case_idx = data_clean.set_index(['emp_name','case_status'])\n",
    "emp_h1bdep_idx = data_clean.set_index(['emp_name','emp_h1b_dependent'])\n",
    "\n",
    "case_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','case_status'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "h1bdep_by_emp = (\n",
    "            emp_case_idx\n",
    "            .groupby(['emp_name','emp_h1b_dependent'])\n",
    "            .count()['case_year']\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    ")\n",
    "\n",
    "case_by_emp_tots = case_by_emp.sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the percentage of each case status (certified or denied) within each category of the specified feature. This plot can help identify any imbalances in the distribution of case statuses across different categories of the feature. For example, if a certain feature category has a much higher percentage of denied cases compared to the overall percentage of denied cases, it may indicate that this category is a risk factor for case denial and vice versa for certified cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "def makebargraph(feature):\n",
    "   \"\"\"\n",
    "   makes a bar graph for binary and categorical features\n",
    "   \"\"\"\n",
    "   counts = data_clean.groupby(feature)['case_status'].value_counts(normalize=True)\n",
    "   counts = counts.mul(100).rename('percent').reset_index()\n",
    "   print(counts)\n",
    "   counts = counts.pivot(index=feature, columns='case_status', values='percent')\n",
    "\n",
    "\n",
    "   ax = counts.plot.bar(stacked=True, rot=0)\n",
    "   ax.set_ylabel('Percent')\n",
    "   ax.legend(title='Case Status')\n",
    "\n",
    "\n",
    "   # Add percentage values to each bar\n",
    "   for container in ax.containers:\n",
    "       ax.bar_label(container, label_type='edge', labels=[f'{val:.2f}%' for val in container.datavalues])\n",
    "\n",
    "\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "makebargraph('emp_h1b_dependent')\n",
    "makebargraph('emp_willful_violator')\n",
    "makebargraph('full_time_position')\n",
    "makebargraph('case_year') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of the prevailing wage can be significant because it provides an overview of the distribution of the wages in the dataset. It helps to understand the range of wages that the H1B visa applicants are earning in the United States. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make histogram for prevailing wage\n",
    "\n",
    "data_clean['prevailing_wage'].hist(bins = 10, range = (0, 300000))\n",
    "plt.xlabel('Prevailing Wage')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "We crafted 5 new features from those that were in our dataset that we felt were relevant. We believe these new features could improve the predictive performance of more sophisticated models. These 5 new features are:\n",
    "- Type of Occupation/SOC Field: this will give a broad occupational category based on the Dept. of Labor's SOC system\n",
    "- Application Duration: the period of time between the application being submitted and the decision being made\n",
    "- Location: general location (ex: SouthWest) that the employer is located in\n",
    "- Yearly Prevailing Wage: expected annual salary for the job position being applied for\n",
    "- Job Demand: Counts number of applications submitted to each employer to test if there is demand for these poisitions offered by employers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOC field\n",
    "\n",
    "data_clean['soc_field'] = data_clean['soc_code'].str[:2]\n",
    "data_clean['soc_code'].unique()\n",
    "#cleaning up the soc_field column\n",
    "#both of the occupations represented by these letters fall under 15, \n",
    "data_clean['soc_field'] = data_clean['soc_field'].replace(['CO', 'SO'], '15') \n",
    "\n",
    "#the occupations represented by these letters fall under 17\n",
    "data_clean['soc_field'] = data_clean['soc_field'].replace(['EL', 'EN', 'ME'], '17')\n",
    "data_clean['soc_field'] = data_clean['soc_field'].replace('1-', '31') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting data_clean to data_final to preserve the original clean dataset. data_final will contain our generated features\n",
    "data_final = data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert prevailing wage to yearly wage based off the pw_unit column (prevailing wage unit)\n",
    "conversion_factors = {'Y':1,'H':2080,'M':12,'W':52,'BW':26}\n",
    "data_final['pw_yearly'] = (\n",
    "        data_final.apply(lambda row: row['prevailing_wage'] * conversion_factors[row['pw_unit']], axis=1)\n",
    "                           )\n",
    "\n",
    "data_final['wage_from_yearly'] = (\n",
    "    data_final.apply(lambda row: row['wage_from'] \n",
    "    * conversion_factors[row['wage_unit']], axis=1)\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploration of any discrepancy between yearly prevailing wage and yearly wage; this was not used in the final dataset\n",
    "wage_diff = (\n",
    "    (\n",
    "    data_final['pw_yearly']-data_final['wage_from_yearly']\n",
    "     )/data_final['wage_from_yearly']\n",
    "    )*1.0\n",
    "\n",
    "wage_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application Duration\n",
    "data_final['application_duration'] = (data_clean['decision_date'] - data_clean['case_submitted']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location\n",
    "def get_location_from_zip(zip_code):\n",
    "    \"\"\"\n",
    "    uses zip_code parameter to find the broader geographical region the employer is in\n",
    "    \"\"\"\n",
    "    if not zip_code.isnumeric() or len(zip_code) < 5:\n",
    "        return \"Unknown\"\n",
    "    region_code = int(zip_code[0])\n",
    "    if region_code == 0 or region_code == 1:\n",
    "        return \"Northeast\"\n",
    "    elif region_code == 7 or region_code == 3 or region_code == 2:\n",
    "        return \"South\"\n",
    "    elif region_code == 4 or region_code == 5 or region_code == 6:\n",
    "        return \"Midwest\"\n",
    "    elif region_code == 8 or region_code == 9:\n",
    "        return \"West\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "data_final['location'] = data_clean['emp_zip'].apply(get_location_from_zip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_demand = data_clean.groupby('emp_name').size().reset_index(name='job_demand')\n",
    "data_final = pd.merge(data, job_demand, on='emp_name', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have constructed our 5 new features, we got rid of redundant features to reduce the dimensional load on future training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "     \"emp_zip\",\n",
    "    \"pw_unit\", \n",
    "    \"wage_unit\",\n",
    "    \"work_city\",\n",
    "  \"work_state\",\n",
    "  \"case_submitted\",\n",
    "  \"decision_date\"\n",
    "]\n",
    "\n",
    "data_final = data_final.drop(drop_cols, axis=1) \n",
    "\n",
    "\n",
    "data_final = data_final.replace('Y',1).replace('N',0)\n",
    "data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models\n",
    "\n",
    "We have two baseline models that we aim to surpass in predictive power in terms of our selected error metrics. \n",
    "\n",
    "The first model is a simple decision stump classifier based on our findings from the confusion matrix between Willful Violator and Case Status. As per the findings, Willful Violator is heavily correlated with Case Status. This simple decision stump classifier is based on the assumption that being a willful violator would result in a rejected application.\n",
    "\n",
    "The second model is a Naive Bayes Classifier selected as a lower bound for multivariate machine learning performance. Primarily selected for its simplicity and training speed, this model also serves as a good baseline to compare more complex models to. \n",
    "\n",
    "Ideally, we surpass our first baselines model in all of our evaluation metrics but we expect to outperform the second baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_decision_stump_classifier(row):\n",
    "    if row['emp_willful_violator']=='N':\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'NC'\n",
    "    \n",
    "dec_stump_preds = data_clean.apply(simple_decision_stump_classifier, axis=1)\n",
    "true = data_clean['case_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSimple Decision Stump Accuracy: \"+accuracy_score(true, dec_stump_preds))\n",
    "print(\"\\nSimple Decision Stump F1 Score: \"+f1_score(true, dec_stump_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we calculate the accuracy and f1_score of the decision stump classifier prediction\n",
    "\n",
    "TP = np.count_nonzero([(true==\"C\") & (dec_stump_preds==\"C\")])\n",
    "FP = np.count_nonzero([(true==\"NC\") & (dec_stump_preds==\"C\")])\n",
    "TN = np.count_nonzero([(true==\"NC\") & (dec_stump_preds==\"NC\")])\n",
    "FN = np.count_nonzero([(true==\"C\") & (dec_stump_preds==\"NC\")])\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# calculate precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# calculate recall\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# calculate f1 score\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeatable random sample of 20000 to avoid memory and time constraints for the checkpoint, if memory permitting, will use larger sample in future\n",
    "df = data_final.sample(n=20000, random_state=42)\n",
    "\n",
    "X=df[[c for c in df.columns if c!='case_status']]\n",
    "\n",
    "y=df['case_status']\n",
    "\n",
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ss_col = [\n",
    "    'wage_from_yearly',\n",
    "    'pw_yearly',\n",
    "    'application_duration'\n",
    "]\n",
    "\n",
    "bin_col = [\n",
    "    \"emp_h1b_dependent\",\n",
    "    \"emp_willful_violator\",\n",
    "    \"full_time_position\"\n",
    "]\n",
    "\n",
    "label_col = [\n",
    "    \"case_year\",\n",
    "]\n",
    "\n",
    "oneh_col = [\n",
    "    \"soc_field\",\n",
    "    \"emp_name\",\n",
    "    \"location\",\n",
    "]\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ss_col),\n",
    "        ('bin', Binarizer(), bin_col),\n",
    "        ('label', OrdinalEncoder(), label_col),\n",
    "        ('oneh', OneHotEncoder(handle_unknown='ignore'), oneh_col)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', nb)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "X_train_processed = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "nb.fit(X_train_processed.toarray(), y_train)\n",
    "\n",
    "# Evaluate the pipeline on the testing data\n",
    "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "accuracy = nb.score(X_test_processed.toarray(), y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "There are several ethics and privacy concerns with the dataset we have chosen regarding H1-B Non Immigrant Labour Visa. The dataset that we have is anonymous, however, with the features described, employees could be re-identified, as the dataset contains information regarding the city, state, and zip code of both the employer and employee. However, to address this, we will refrain from using information of the employee in our project. The dataset also avoids proxy discrimination, as we are not using features such as race, origin, sex, and age, rather using attributes from the company. As far as bias, since we are mainly going to be using columns regarding the employer and the case status, and disregarding columns related to the employee. As we are reducing the number of columns used, we could be missing some potential variables that could lead to biased predictions in our results regarding the approval of visas. To address this we will use evaluation metrics that mitigate bias. In terms of the impact of our work, it's important to consider the potential consequences of the decisions that are made based on your analysis. For example, if our analysis suggests that certain employers are more likely to receive H1-B visas than others, this could have a significant impact on the job market and on the lives of the people involved. It's therefore important to take steps to mitigate any potential biases in our analysis, and to ensure that your work is fair, accurate, and ethical.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Footnotes\n",
    "<a name=\"mpinote\"></a>1.[^](#mpinote): Migration Policy Institute. (2022). H-1B Temporary Skilled Worker Program. Retrieved February 22, 2023, from https://www.migrationpolicy.org/article/h-1b-temporary-skilled-worker-program.<br> \n",
    "<a name=\"iqbalnote\"></a>2.[^](#iqbalnote): M. Tariq Iqbal, M. Irfan Ullah, M. Ahsan Nazir, \"A Hybrid Algorithm for Optimal Placement and Sizing of Distributed Generation in a Distribution System\", IEEE Access, vol. 7, pp. 174089-174102, 2019. doi: 10.1109/ACCESS.2019.2953292. https://ieeexplore.ieee.org/abstract/document/8933628<br>\n",
    "<a name=\"rananote\"></a>3.[^](#rananote): Rana, P., Kumar, S., & Yadav, S. (2019). An allotment of H1B work visa in USA using machine learning. International Journal of Recent Technology and Engineering, 8(3S), 539-545. Available from: https://www.researchgate.net/profile/Prashant-Rana-4/publication/328488339_An_allotment_of_H1B_work_visa_in_USA_using_machine_learning/links/5d70f092a6fdcc9961afad48/An-allotment-of-H1B-work-visa-in-USA-using-machine-learning.pdf.<br>\n",
    "<a name=\"paulnote\"></a>4.[^](#paulnote): Paul, U., & Langlois, A. (2022). Understanding the Determinants of H-1B Decisions: A Machine Learning Approach. Northeastern University, College of Social Sciences and Humanities. Retrieved February 22, 2023, from https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4143882<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "19b17eba0dbd5e4b8827ab8a6192fc0dff7c2985f63f4f278d5b971ef380745d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
